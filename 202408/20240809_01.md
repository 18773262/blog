## ollama新上架了3个embedding模型  
                                                                    
### 作者                                        
digoal                                        
                                               
### 日期                                             
2024-08-09                                        
                                            
### 标签                                          
PostgreSQL , PolarDB , DuckDB , 大模型 , rag , 文本 , 向量化 , embedding , vector                
                                                                   
----                                            
                                                          
## 背景      
ollama新上架了3个embedding模型  
  
1、BGE-M3  
  
https://ollama.com/library/bge-m3  
  
BGE-M3 是BAAI的一款新模型，以其在 多功能性、多语言性、多粒度 方面而闻名。  
    
BGE-M3基于XLM-RoBERTa架构，以其多功能性、多语言性和多粒度而闻名：  
- 多功能：它可以同时支持嵌入模型的三个常见检索功能：密集检索、多向量检索和稀疏检索。  
- 多语言：它可以支持100多种工作语言。  
- 多粒度：它能够处理不同粒度的文字输入，从短句到多达8192个令牌的长文档。  
  
2、bge-large  
  
https://ollama.com/library/bge-large  
  
  
BAAI发布的向量化模型, 将文本映射到向量空间。  
  
FlagEmbedding可以将任何文本映射到低维密集向量，该向量可用于检索、分类、聚类或语义搜索等任务。它也可以存入向量数据库用于LLM的RAG场景。  
  
  
3、paraphrase-multilingual  
  
https://ollama.com/library/paraphrase-multilingual  
  
这是一个句子变换器模型：它将句子和段落映射到768维的密集向量空间，可用于聚类或语义搜索等任务。  
  
  
用法举例  
  
https://github.com/ollama/ollama/blob/main/docs/api.md#generate-embeddings  
  
调用  
```  
curl http://localhost:11434/api/embed -d '{  
  "model": "bge-large:335m",  
  "input": "Why is the sky blue?"  
}'  
```  
  
返回  
```  
{"model":"bge-large:335m","embeddings":[[0.0013840914,-0.0022069444,  
...  
,-0.011367752]],"total_duration":1840892375,"load_duration":1778941666,"prompt_eval_count":8}  
```  
  
也可以在python脚本中进行调用, 参考如下RAG相关文章:  
- [《AI大模型+全文检索+tf-idf+向量数据库+我的文章 系列之2 - 我的数字人: 4000余篇github blog文章投喂大模型中》](../202407/20240719_01.md)    
  
    
