## 影响性能和可靠性的大模型(LLM)常见参数设置
                                                              
### 作者                                  
digoal                                  
                                         
### 日期                                       
2024-07-31                                  
                                      
### 标签                                    
PostgreSQL , PolarDB , DuckDB , 大模型 , 参数设置 , temperature , top_p         
                                                             
----                                      
                                                    
## 背景     


https://www.promptingguide.ai/zh/introduction/settings

https://github.com/ollama/ollama/blob/main/docs/modelfile.md#parameter

https://github.com/ollama/ollama/tree/main/docs




>>> /?
Available Commands:
  /set            Set session variables
  /show           Show model information
  /load <model>   Load a session or model
  /save <model>   Save your current session
  /clear          Clear session context
  /bye            Exit
  /?, /help       Help for a command
  /? shortcuts    Help for keyboard shortcuts

Use """ to begin a multi-line message.

>>> /set
Available Commands:
  /set parameter ...     Set a parameter
  /set system <string>   Set system message
  /set history           Enable history
  /set nohistory         Disable history
  /set wordwrap          Enable wordwrap
  /set nowordwrap        Disable wordwrap
  /set format json       Enable JSON mode
  /set noformat          Disable formatting
  /set verbose           Show LLM stats
  /set quiet             Disable LLM stats

>>> /set parameter
Available Parameters:
  /set parameter seed <int>             Random number seed
  /set parameter num_predict <int>      Max number of tokens to predict
  /set parameter top_k <int>            Pick from top k num of tokens
  /set parameter top_p <float>          Pick token based on sum of probabilities
  /set parameter num_ctx <int>          Set the context size
  /set parameter temperature <float>    Set creativity level
  /set parameter repeat_penalty <float> How strongly to penalize repetitions
  /set parameter repeat_last_n <int>    Set how far back to look for repetitions
  /set parameter num_gpu <int>          The number of layers to send to the GPU
  /set parameter stop <string> <string> ...   Set the stop parameters

>>> /show
Available Commands:
  /show info         Show details for this model
  /show license      Show model license
  /show modelfile    Show Modelfile for this model
  /show parameters   Show parameters for this model
  /show system       Show system message
  /show template     Show prompt template

>>> /show parameters
Model defined parameters:
stop                           "<|start_header_id|>"
stop                           "<|end_header_id|>"
stop                           "<|eot_id|>"



| Parameter      | Description                                                                                                                                                                                                                                             | Value Type | Example Usage        |
| -------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------- | -------------------- |
| mirostat       | 启用 Mirostat 采样来控制困惑度。 (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0)                                                                                                                                         | int        | mirostat 0           |
| mirostat_eta   | 生成文本反馈的响应速度, 较低的值导致较慢的生成响应速度，较高的值导致较快的生成响应速度。 (Default: 0.1)                        | float      | mirostat_eta 0.1     |
| mirostat_tau   | 控制输出的连贯性和多样性之间的平衡。值越低，文本内容越聚焦、越连贯。(Default: 5.0)                                                                                                         | float      | mirostat_tau 5.0     |
| num_ctx        | 生成下一个token时, 在已生成内容中, 从当前位置回顾上下文窗口的大小。 (Default: 2048)                                                                                                                                                                    | int        | num_ctx 4096         |
| repeat_last_n  | 模型回溯多远(上下文窗口)以防止生成重复token。 (Default: 64, 0 = disabled, -1 = num_ctx)                                                                                                                                           | int        | repeat_last_n 64     |
| repeat_penalty | 对生成重复token的惩罚力度。较高的值（例如 1.5）将对重复的惩罚力度更大，而较低的值（例如 0.9）将更宽松。（默认值：1.1）                                                                     | float      | repeat_penalty 1.1   |
| temperature    | 模型的温度。增加温度将使模型的回答更具创意(开放), 适合需要创意的工作例如写作. 更低的温度使得回答更加精准, 适合有标准答案的场景。 (Default: 0.8)                                                                                                                                     | float      | temperature 0.7      |
| seed           | 设置用于生成的随机数种子。将其设置为特定数字将使模型针对同一提示生成相同的文本。（默认值：0）                                                                                      | int        | seed 42              |
| stop           | 设置要使用的停止词。遇到此停止词时，LLM 将停止生成文本并返回。可以通过`stop`在`modelfile`中指定多个单独的参数来设置多个停止模式。                                     | string     | stop "AI assistant:" | 
| tfs_z          | 减少输出中 “可能性较小的token” 的影响 (对不关键信息的减权, 例如生成token参考的上下文中的不关键信息)。较高的值（例如 2.0）将进一步减少影响，而值 1.0 则禁用此设置。                                              | float      | tfs_z 1              |
| num_predict    | 生成文本时要预测的最大token数。 (Default: 128, -1 = infinite generation, -2 = fill context)                                                                                                                                   | int        | num_predict 42       |
| top_k          | 降低产生无意义答案的概率。值越高（例如 100）答案就越多样化，值越低（例如 10）答案就越保守。（默认值：40）                                                                       | int        | top_k 40             |
| top_p          | 与 top-k 配合使用。较高的值（例如 0.95）将产生更加多样化的文本，而较低的值（例如 0.5）将产生更加集中和保守的文本。（默认值：0.9）                                                                  | float      | top_p 0.9            |
| min_p          | top_p 的替代方案，旨在确保质量和多样性的平衡。参数p表示相对于最可能标记的概率，考虑标记的最小概率。例如，当p =0.05 且最可能标记的概率为 0.9 时，值小于 0.045 的 logit 将被过滤掉。（默认值：0.0） | float      | min_p 0.05            |

