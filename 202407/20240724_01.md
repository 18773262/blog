


https://apeatling.com/articles/simple-guide-to-local-llm-fine-tuning-on-a-mac-with-mlx/

https://apeatling.com/articles/part-1-setting-up-your-environment/

https://apeatling.com/articles/part-2-building-your-training-data-for-fine-tuning/

https://apeatling.com/articles/part-3-fine-tuning-your-llm-using-the-mlx-framework/

https://apeatling.com/articles/part-4-testing-and-interacting-with-your-fine-tuned-llm/

你好！您是否想在 Apple Silicon Mac 上微调大型语言模型 (LLM)？如果是这样，那么您来对地方了。

让我们一步一步地介绍微调过程。这不会花你一分钱，因为我们将使用[Apple 的 MLX 框架](https://github.com/ml-explore/mlx)在您自己的硬件上完成所有操作。

一旦我们完成，您将拥有一个完全优化的 LLM，您可以在自己的设备上轻松完成所有操作。

我把本指南分成了多个部分。每个部分都是独立的，所以你可以直接跳到与你最相关的部分：
- 设置您的环境。
- 构建训练数据以进行微调。
- 使用 MLX 框架微调您的 LLM。
- 测试并与您微调的 LLM 进行交互。

## 第 1 部分：设置你的环境

如果您是从零开始，则需要在 Mac 上设置一些东西才能开始。如果您已准备好 Python 和 Pip，则可以跳至第二步。

步骤 1：安装 Homebrew
[Homebrew](https://brew.sh/)是一个包管理器，可以帮助你管理和安装所需的软件（例如 Python）。我喜欢它，因为它提供了一种切换版本和卸载包的简单方法。

要安装 Homebrew，请打开终端应用程序（或安装 iTerm）并粘贴以下内容：

/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"


如果安装过程中提示您，请继续安装 Xcode 命令行工具。

第 2 步：安装升级版本的 Python 和 Pip
从 Mac OS 12 Sonoma 开始，Mac OS 已预装 Python 3。我已升级到 3.11.x 以获取最新的修复和安全更新。

我会坚持使用最新版本减去一个主要版本，否则您可能会遇到不兼容的库问题。因此，如果 3.12 是最新版本，请从 3.11 版本中选择最新版本。

要安装 Python 3.11 的最新次要版本，请使用以下命令：
brew install python@3.11

完成后，您将安装 python 3.11.x 及其包管理器 pip。

一个可选步骤是将python3.11和pip3.11命令别名为python和pip，这样在遵循示例时会更简单一些。版本更改时，您需要更改这些别名。

您可以使用以下命令为它们添加别名（如果您安装了较新的版本，请使用较新的版本号）：

将 python 和 pip 别名添加到 zsh 配置的命令
echo '
alias python=python3.11
alias pip=pip3.11
' >> ~/.zshrc

这就是环境，现在您可以进入第 2 部分：构建用于微调的训练数据。

## 第 2 部分：构建用于微调的训练数据

有许多不同的工具可以在 Mac 上本地运行 LLM。[LM Studio](https://lmstudio.ai/)是一个很好的工具，它提供了一个不错的 UI 来运行和与离线 LLM 聊天。在本指南中，我将使用[Ollama](https://ollama.ai/)，因为它提供了一个本地 API，我们将使用它来构建微调训练数据。

步骤 1：下载 Ollama 并提取模型
继续下载并安装 [Ollama](https://ollama.ai/download)。在本指南中，我将使用[Mistral.ai](https://mistral.ai/)的[Mistral 7B Instruct v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) 模型。如果您愿意，可以拉取不同的模型，只需从现在开始将所有内容切换为您自己的模型即可。

要拉取模型，请使用以下命令：
拉下 Mistral 7B 并使用默认的指导模型。
ollama pull mistral:7b

一旦有了模型，您可以选择使用命令run来pull尝试从命令行与其进行交互。

第 2 步：确定正确的训练数据格式
为了微调 Mistral 7B，我们需要训练数据。训练数据将使微调后的模型能够产生比单靠提示更高质量的结果。您的训练数据应该充满了您希望在微调后看到的结果类型的示例。

Mistral 7B 的训练数据应采用[JSONL 格式](https://jsonlines.org/)，每行格式如下：

JSONL 格式的单行训练数据。
{"text":"<s>[INST] Instruction[/INST] Model answer</s>[INST] Follow-up instruction[/INST]"}

如果您正在训练不同的模型，格式可能会有所不同 - 因此请务必检查。

如果您在电子表格、数据库或其他存储中已有训练数据，那么您的目标是将其转换为上述 JSONL 格式。如果您已完成此操作，则可以继续进行微调步骤。

步骤 3：创建生成说明的提示
我手头没有任何好的训练数据，所以我决定生成它。我想微调模型，以便对有关使用 WordPress 网站的问题提供更广泛的答案。

我使用了 Ollama，运行了一系列不同的模型，并使用了以下提示：

Please list in JSON format 50 frequently asked questions about WordPress from all levels of users working on WordPress websites.  

The questions should start with any of the following: “Where do I", "Is it okay to", "Can you help me", "I need to", "Is there a", "Do you know", "Where is the", "Can you tell me", "Can I change", "What are the", "How do I", "When is it", "Does WordPress have", "How to", "What is the difference", "Can users", "Can I", "What is”.  

You do not need to provide an answer or category to each question. The list should be a single dimension array of only questions.

经过反复尝试才得到这个提示。要求以特定短语开头的说明有助于改善生成问题的范围。

最好花一些时间在这里找出最佳提示，以便生成最适合您需求的指令列表。

步骤 4：使用 Ollama 生成指令
一旦你有了良好的生成提示，你就可以通过 Ollama 运行它并切换你使用的[模型](https://ollama.ai/library)。对于每组指令，我将它们添加到单个 JSON 文件中。

以下是 Mistral 7B 的截断结果：

❯ ollama run mistral:7b
>>> Please list in JSON format 50 frequently asked questions about WordPress from all levels of users working on WordPress websites.
...
... The questions should start with any of the following: “Where do I", "Is it okay to", "Can you help me", "I need to", "Is there a",
... "Do you know", "Where is the", "Can you tell me", "Can I change", "What are the", "How do I", "When is it", "Does WordPress have",
... "How to", "What is the difference", "Can users", "Can I", "What is”.
...
... You do not need to provide an answer or category to each question. The list should be a single dimension array of only questions.
 [
"Where do I find the WordPress login url?",
"Is it okay to install WordPress on a local server?",
"Can you help me recover a lost WordPress password?",
"I need to update my WordPress version, how do I do that safely?",
"Is there a limit to the number of plugins I can install?",
"Do you know how to create custom post types in WordPress?",
"Where is the database location for a WordPress site?",
"Can you tell me how to add custom CSS to my WordPress site?",
"Can I change the permalink structure in WordPress?"

[...]


人们普遍认为，至少需要 50-100 个示例才能通过微调获得有利结果。我无法说出理想的数量是多少，但我想你会想要更多。这是我仍需要尝试的事情。

我继续生成了 1,200 条指令。下次我将编写一个脚本来执行此操作，而不是手动将它们添加到 JSON 文件中。

将所有问题都放在一个 JSON 文件中。保存此文件。我将其命名为`instructions.json`。

第 5 步：根据你的指示生成模型答案
现在您拥有所有指令的 JSON 文件，您可以使用 [Ollama API](https://github.com/jmorganca/ollama/blob/main/docs/api.md)为每个指令生成模型答案。

为此，我编写了一个非常简单的 Python 脚本，可以在命令行上运行该脚本来查询 Ollama API 并生成 JSONL 训练文件。

将其保存为generate.py文件。我在 [Github](https://github.com/apeatling/beginners-guide-to-mlx-finetuning/tree/trunk) 上有一个完整的示例副本，您还可以在其中找到该脚本的 其他 版本。

import json
import requests
import sys
from pathlib import Path

def query_ollama(prompt, model='mistral', context=''):
    url = 'http://localhost:11434/api/generate'
    data = {"model": model, "stream": False, "prompt": context+prompt}
    response = requests.post(url, json=data)
    response.raise_for_status()
    followup_data = {"model": model, "stream": False, "prompt": response.json()['response'].strip() + "What is a likely follow-up question or request? Return just the text of one question or request."}
    followup_response = requests.post(url, json=followup_data)
    followup_response.raise_for_status()
    return response.json()['response'].strip(), followup_response.json()['response'].replace("\"", "").strip()

def create_validation_file(train_file, valid_file, split_ratio):
    with open(train_file, 'r') as file:
        lines = file.readlines()
    valid_lines = lines[:int(len(lines) * split_ratio)]
    train_lines = lines[int(len(lines) * split_ratio):]
    with open(train_file, 'w') as file:
        file.writelines(train_lines)
    with open(valid_file, 'w') as file:
        file.writelines(valid_lines)

def main(instructions_file, train_file, valid_file, split_ratio):
    if not Path(instructions_file).is_file():
        sys.exit(f'{instructions_file} not found.')

    with open(instructions_file, 'r') as file:
        instructions = json.load(file)

    for i, instruction in enumerate(instructions, start=1):
        print(f"Processing ({i}/{len(instructions)}): {instruction}")
        answer, followup_question = query_ollama(instruction)
        result = json.dumps({
            'text': f'<s>[INST] {instruction}[/INST] {answer}</s>[INST]{followup_question}[/INST]'
        }) + "\n"
        with open(train_file, 'a') as file:
            file.write(result)
    
    create_validation_file(train_file, valid_file, split_ratio)
    print("Done! Training and validation JSONL files created.")

if __name__ == "__main__":
    if len(sys.argv) != 5:
        sys.exit("Usage: python script.py <instructions.json> <train.jsonl> <valid.jsonl> <split_ratio>")
    main(sys.argv[1], sys.argv[2], sys.argv[3], float(sys.argv[4]))


该脚本将允许您根据要用于获取答案的模型来更改模型。您甚至可以切换模型并从不同的模型生成答案。

要运行脚本，您可以调用它 `python3 generate.py instructions.json train.jsonl valid.jsonl 0.6` , 相当于通过instructions汇总的问题生成有答案的结果集, 并把结果集拆成2个部分, `train.jsonl`用于训练, `valid.jsonl`用于调教/拟合训练结果, 类似监督学习的过程. 


生成完成后，您将获得两个新文件：`train.jsonl`和`valid.jsonl`。我们将在第三部分中使用这些文件和Apple 的 MLX 框架在您的 Mac 上启动本地模型训练。

## 第 3 部分：使用 MLX 框架微调你的 LLM

步骤 1：克隆 mlx-examples Github 存储库
苹果公司的机器学习研究人员创建了一个名为 [MLX](https://github.com/ml-explore/mlx) 的出色新框架。MLX 是一个专为 Apple 芯片构建的阵列框架，允许其利用 Mac 中的统一内存，从而带来显着的性能改进。

[mlx-examples](https://github.com/ml-explore/mlx-examples) 包含一系列使用 MLX 的不同示例。这是学习如何使用它的好方法。

我假设您了解 Git 和 Github。您可以使用以下方法克隆 MLX 示例存储库：

git clone https://github.com/ml-explore/mlx-examples.git

在本指南中，我们将重点关注 [LoRA（低秩自适应）](https://huggingface.co/papers/2106.09685) [示例](https://github.com/ml-explore/mlx-examples/tree/main/lora)，我们将使用这些示例来微调 Mistral 7B Instruct 并创建我们的微调适配器。

第 2 步：使用训练数据开始微调过程
在 MLX 的早期版本中，您需要将模型转换为与 MLX 兼容。现在不再需要这样做了，您可以从 Hugging Face 中指定一个模型并立即开始使用它进行微调。

我们可以使用上一步的train.jsonl和文件来启动训练程序。valid.jsonl

第一步是前往mlx-examples/lora文件夹并安装 lora 对话脚本的要求：

cd mlx-examples/lora
pip install -r requirements.txt
移动到 lora 文件夹并安装要求。
一旦安装了要求，我们就可以使用lora.py脚本来启动训练。

有几个参数需要了解。你可以运行带有--help参数的脚本来了解更多信息，或者查看 Github 上的文档。

这是我用来训练和开始微调的确切命令：

python lora.py \
  --train \
  --model mistralai/Mistral-7B-Instruct-v0.2 \
  --data ~/Developer/AI/Models/fine-tuning/data \
  --batch-size 2 \
  --lora-layers 8 \
  --iters 1000
启动微调过程的命令。
该--model参数应指向Hugging Face 上模型的路径。如果您有本地模型，则可以提供本地 MLX 转换模型的模型目录。有关转换过程的更多信息，请查看转换文档。

该--data参数是可选的，应该指向包含您的train.jsonl和valid.jsonl文件的文件夹，默认情况下它将在当前目录中查找。

您需要尝试使用batch-size和lora-layers参数。一般来说，系统内存越大，这些数字就越高。文档对此有一些很好的提示。

这里的参数--iters并非绝对必要，默认值为 1000 次迭代。我之所以包含它，是因为您可能仍想尝试一下。

第四步：坐下来，听听电脑风扇的声音

一旦你启动了训练过程，它会给你关于你的训练、验证损失、令牌和每秒迭代次数的有用反馈。

使用我的 M1 Max 和 32GB 系统内存、批处理大小 1 和 LoRA 层 4，我平均可以得到大约 260 个令牌/秒。使用上述设置，我最终平均得到接近 120 个令牌/秒：

Loading pretrained model
Total parameters 7242.584M
Trainable parameters 0.852M
Loading datasets
Training
Iter 1: Val loss 0.868, Val took 94.857s
Iter 10: Train loss 0.793, It/sec 0.223, Tokens/sec 229.758
Iter 20: Train loss 0.683, It/sec 0.293, Tokens/sec 218.803
Iter 30: Train loss 0.768, It/sec 0.072, Tokens/sec 94.586
Iter 40: Train loss 0.686, It/sec 0.081, Tokens/sec 78.442
Iter 50: Train loss 0.734, It/sec 0.230, Tokens/sec 191.031
Iter 60: Train loss 0.600, It/sec 0.043, Tokens/sec 85.585
Iter 70: Train loss 0.691, It/sec 0.027, Tokens/sec 33.718
Iter 80: Train loss 0.568, It/sec 0.031, Tokens/sec 46.508

[...]
随着时间的推移，模型训练输出。
我也尝试在具有 16GB 系统内存的 M1 MacBook Air 上进行训练。它显然速度较慢，但如果和的值较低batch-size且lora-layers没有其他程序运行，则应该没问题。您还可以尝试使用 4 位量化来减少内存需求。

根据基准测试，具有更多内存的新系统将运行得更快。

步骤 5：找到您的 adapters.npz
微调完成后，您会adapters.npz在开始训练的文件夹中找到一个文件。此文件包含微调后对基础模型进行的额外权重更改，您将在最后一步中提供此文件的位置：测试和与微调后的 LLM 交互。


## 第 4 部分：测试并与经过微调的 LLM 进行交互
步骤 1：确定提示以比较微调结果
为了测试并查看微调模型的结果，您需要将微调模型与基础模型进行比较。

我不会详细介绍实现此目的的最佳方法，您可能希望为试图通过微调来改进的输出创建一些提示。

根据您进行的微调类型，您可以采用定量或定性的方法来评估结果。

第 2 步：在基础模型和微调模型上测试你的提示
MLX 库具有一些内置功能来处理模型推理。

要启动基础模型，请前往repollms内的文件夹mlx-examples。您可以使用以下mlx_lm.generate命令启动基础模型：

python -m mlx_lm.generate \  --model mistralai/Mistral-7B-Instruct-v0.2 \  --max-tokens 1000 \  --prompt "How do I build an online store using WordPress that will allow me to sell car parts? I want to be able to ship them and charge people using credit cards."
提示基础模型的命令。
为了提示您的微调模型，您需要提供适配器文件的位置。您可以从repolora中的文件夹中执行此操作mlx-examples。使用lora.py脚本和以下参数：

python lora.py \  --model mistralai/Mistral-7B-Instruct-v0.2 \  --adapter-file ./fine-tuning/adapters/adapters.npz \  --num-tokens 1000 \  --prompt "How do I build an online store using WordPress that will allow me to sell car parts? I want to be able to ship them and charge people using credit cards."
提示您微调模型的命令。
--model在上述两个命令中，请确保将和参数替换--adapter-file为适合您设置的正确的 Hugging Face 路径和适配器文件位置。

该num-tokens参数是可选的，默认为 1000。根据您预期的答案长度，您可能需要对其进行调整。

步骤 3：分析结果
我对 Mistral 7B 的 WordPress 微调只是一个有趣的练习，所以我没有准备对结果进行非常彻底的分析。

我尝试了 10 到 20 个不同的提示，其中包含更复杂的 WordPress 问题，并评估了与基本模型相比答案的详尽程度和完整性。

正如我上面提到的，根据您想要的结果类型，您可能对如何评估有具体的想法。这取决于你。

为了完整起见，以下是基本模型的提示输出，以及上述示例中使用的提示的微调模型。您可以看到答案的深度和质量已显著提高，但响应长度也大幅增加。还需要做更多的工作！

步骤 4：将适配器融合到新模型中
如果您对新的微调模型感到满意，您可以继续将 LoRA 适配器文件融合回基础模型。

这样做可以更轻松地使用传统推理工具运行模型（尽管GGUF 格式尚未完全准备好）。如果您愿意，您还可以将经过微调的模型上传回Hugging Face MLX 社区。

要融合您的模型，请使用文件夹中的以下内容lora：

python fuse.py \  --model mistralai/Mistral-7B-Instruct-v0.2 \  --adapter-file ./fine-tuning/adapters/adapters.npz \  --save-path ./Models/My-Mistral-7B-fine-tuned  --upload-name My-Mistral-7B-fine-tuned  --de-quantize 
将适配器与基本模型融合的命令。
如果您的适配器文件位于同一目录中，并且您想要将新模型保存到同一目录，则adapter-file和参数是可选的。save-path

如果您不想将微调后的模型上传至 Hugging Face，您可以省略该upload-name参数，它将仅保存到您的save-path位置（如果您省略此参数，则保存到同一文件夹）。

下一步，我们将把您的微调模型转换为 GGUF 格式。为此，我们必须使用参数进行反量化--de-quantize。如果您不想转换为 GGUF，则可以省略此参数。

最后，如果您从本地基础 MLX 转换模型开始，请提供文件夹位置作为参数model。在这种情况下，您需要在参数中提供原始 Hugging Face 模型路径，hf-path以将微调后的版本上传到 Hugging Face。

和我的模特聊天怎么样？
使用 MLX 示例库中的工具进行提示是一回事，但最明显的问题是“如何与我的微调模型进行持续对话”？

答案是将其转换为 GGUF 格式。虽然 mlx-examples repo 没有内置方法来执行此操作，但您可以转换在上一步中融合的微调模型。

为此，您需要按照使用 llama.cpp 转换为 GGUF 的说明进行操作。一旦您拥有模型的 GGUF 版本，您就可以在 Ollama、LM Studio 和其他应用中运行推理。
