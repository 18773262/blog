## 我的数字人即将诞生: 4000余篇github blog文章已喂给大模型  
  
### 作者  
digoal  
  
### 日期  
2024-07-19  
  
### 标签  
PostgreSQL , PolarDB , DuckDB , AI , macOS , ollama , docker , 数字人 , 大模型  
  
----  
  
## 背景  
系列文章: 
- [《我的数字人: 4000余篇github blog文章投喂大模型中》](../202407/20240719_01.md)  
- [《低配macbook成功跑AI大模型`LLama3:8b`, 感谢ollama》](../202407/20240718_01.md)  
  
经过十几年坚持不懈的努力, 我在github已经发表了4000余篇文章, 把这些文章喂给模型后回答一些专业问题, 会不会有更好的效果呢?  
  
请先参考如下文章在macOS中部署ollam: [《低配macbook成功跑AI大模型`LLama3:8b`, 感谢ollama》](../202407/20240718_01.md)  
  
## 一、用我的文章来喂模型  
为了方便使用, 写个python脚本. 嘿嘿, 下面的python脚本也是ollama生成我简单调整了一下.  
  
安装ollama模块  
```  
pip3 install ollama  
```  
  
这个python脚本输入问题, 以及相关的参考文本, 让模型参考文本进行回答.  
```  
vi exp.py  
  
import ollama  
import sys  
  
def read_files(file_paths):  
    file_contents = []  
    for file_path in file_paths:  
        with open(file_path, 'r') as f:  
            for _ in range(14):  
                next(f)  
            file_contents.append(f.read())  
    return '\n'.join(file_contents)  
  
# Get the file path from the command line argument  
if len(sys.argv) < 3:  
    print("Usage: python script.py [prompt] [file1] [file2] ... ")  
    sys.exit(1)  
  
  
try:  
    prompt = sys.argv[1]  
    file_paths = sys.argv[2:]  
    data = read_files(file_paths)  
  
    output = ollama.generate(  
        model="llama3:8b",  
        prompt=f"请先学习后面提供的参考信息, 使用中文回答这个问题(回答前请先复述问题): {prompt} . 以下全部都是参考信息: {data}"  
    )  
  
    print(output['response'])  
  
except Exception as e:  
    print(f"Error: {e}")  
```  
  
  
使用举例, 让模型参考 “## 重新发现PostgreSQL之美 - 9 面向多值列的倒排索引GIN|RUM” 的内容来进行回答.  
```  
python3 ./exp.py "PostgreSQL gin和rum索引的差别是什么? 列举一些SQL例子, 最后做个总结." /Users/digoal/blog/202105/20210531_02.md  
```  
  
模型回答如下  
  
根据提供的参考信息，我将回答这个问题。  
  
问题：关于 PostgreSQL 的 RUM 模块和 rum_anyarray_addon_ops 操作符的使用和示例。  
  
答案：  
  
RUM（Range Unified Manipulation）是 PostgreSQL 中的一个模块，用于对TSVECTOR和ANYARRAY类型进行范围搜索。rum_anyarray_addon_ops 是 rum 模块中的一个操作符，可以用于将ANYARRAY元素存储到索引中，并支持 prefix 搜索。  
  
示例代码：  
```sql  
create extension rum;  
create table tbl (id serial8 primary key, a int[], n int, crt_time timestamp);  
create index idx_tbl_1 on tbl using rum (a rum_anyarray_addon_ops, n) with (attach='n', to='a');  
  
explain (analyze, verbose, timing, costs, buffers) select * from tbl where a @> array[1,2,3] and n <= 1;  
```  
这段代码首先创建了 RUM 扩展，然后创建了一张名为 `tbl` 的表，其中包含一个整数数组列 `a` 和一个 timestamp 列 `crt_time`。接着，使用 rum 模块创建了一个索引 `idx_tbl_1`，用于对 `a` 列和 `n` 列进行范围搜索。  
  
在查询语句中，我们使用了 `@>` 操作符来搜索数组 `a` 中包含 1、2 和 3 的元素，并且要求 `n` 小于或等于 1。结果显示， PostgreSQL 使用的是索引 `idx_tbl_1` 来执行查询，而不是全表扫描。  
  
总的来说，rum_anyarray_addon_ops 操作符可以帮助我们快速地对 ANYARRAY 类型进行范围搜索，并且可以减少查询时间和提高数据库性能。  
  
## 二、会话记忆 & 外脑知识库  
上面是需要我们自己去找相关文章, 然后告诉模型, 能不能让模型自己去找相关文章呢?  
  
还有一种情况是, 模型在交互过程中, 怎么把好的问题和答案收集起来, 方便回答时有上下文的记忆呢? 而不是每次都要你提醒上一次问的是什么.  
  
基于以上问题, 我们需要用到向量数据库.  
  
未来可以把好的答案写入向量数据库、好的参考信息写入向量数据库, 根据会话ID和相似向量提取文本, 作为输入再反馈给模型, 实现模型记忆与外脑知识库.  
  
如何把文本向量化?  
  
再次感谢ollama, 已经集成了小型embedding模型(不到1GB, 照样是先pull模型然后就可以使用了.):  
- https://ollama.com/blog/embedding-models  
- https://ollama.com/library/mxbai-embed-large  
  
在macOS本机安装postgresql, https://www.postgresql.org/ftp/source/  
```  
1、安装依赖包  
  
export HOMEBREW_API_DOMAIN="https://mirrors.tuna.tsinghua.edu.cn/homebrew-bottles/api"  
export HOMEBREW_BOTTLE_DOMAIN="https://mirrors.tuna.tsinghua.edu.cn/homebrew-bottles"  
export HOMEBREW_BREW_GIT_REMOTE="https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/brew.git"  
export HOMEBREW_CORE_GIT_REMOTE="https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/homebrew-core.git"  
export HOMEBREW_PIP_INDEX_URL="https://pypi.tuna.tsinghua.edu.cn/simple"  
echo 'export HOMEBREW_BOTTLE_DOMAIN="https://mirrors.tuna.tsinghua.edu.cn/homebrew-bottles"' >> ~/.bash_profile  
export HOMEBREW_BOTTLE_DOMAIN="https://mirrors.tuna.tsinghua.edu.cn/homebrew-bottles"  
  
brew install openssl  
brew install pkg-config  
  
2、下载安装icu:  
https://github.com/unicode-org/icu/releases  
  
解压后进入 $SRC/icu4c/source 目录安装  
  
./configure --prefix=/usr/local/icu4c  
make -j 4  
sudo make install  
  
3、下载安装PostgreSQL 16  
  
cd ~/Downloads  
curl https://ftp.postgresql.org/pub/source/v16.3/postgresql-16.3.tar.bz2 -o postgresql-16.3.tar.bz2  
  
tar -jxvf postgresql-16.3.tar.bz2  
cd postgresql-16.3  
export PKG_CONFIG_PATH=/usr/local/icu4c/lib/pkgconfig/  
CC=clang CXX=clang++ ./configure --prefix=/Users/digoal/pg16 --with-ssl=openssl  
make world -j 4  
make install-world  
  
4、配置环境变量  
  
vi ~/pg16env.sh  
export PGHOME=/Users/digoal/pg16  
export DYLD_LIBRARY_PATH=$PGHOME/lib:$PGHOME/lib/postgresql:$DYLD_LIBRARY_PATH  
export PGHOST=127.0.0.1  
export PGPORT=1921  
export PGUSER=postgres  
export PGDATABASE=postgres  
export PATH=$PGHOME/bin:$PATH  
  
echo ". /Users/digoal/pg16env.sh" >> ~/.bash_profile  
  
. ~/.bash_profile  
  
which psql  
  
/Users/digoal/pg16/bin/psql  
```  
  
在macOS本机安装psycopg2  
```  
pip3 config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple  
  
/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip  
  
  
CPPFLAGS="-I/opt/homebrew/include/openssl" LDFLAGS="-L/opt/homebrew/lib" pip3 install psycopg2  
```  
  
使用以下镜像拉起PostgreSQL, 内置vector插件用法请参考: https://github.com/pgvector/pgvector  
- [《2023-PostgreSQL Docker镜像学习环境 ARM64版, 已集成热门插件和工具》](../202308/20230814_02.md)  
  
进入容器后, 建表如下:  
```  
psql  
  
alter role postgres encrypted password '123';  
create extension vector;  
create table tbl (  
  id serial8 primary key,  
  prompt text,  
  answer text,  
  v_prompt vector,  
  v_answer vector  
);  
  
-- 向量索引请参考手册创建  
```  
  
在macOS本机可连接映射端口  
```  
docker ps -a --no-trunc  
  
0.0.0.0:55008->1921/tcp  
```  
  
pull 小型embedding模型
```
ollama pull mxbai-embed-large
```
  
编写一个python脚本, 用于提取blog的markdown文件的标题以及内容, 写入向量数据库中.  
```  
vi exp1.py  
  
  
import psycopg2  
import sys  
import ollama  
  
# Define the PostgreSQL connection settings  
HOST = 'localhost'  
PORT = 55008  
DBNAME = 'postgres'  
USER = 'postgres'  
PASSWORD = '123'  
  
def read_file(file_name):  
    try:  
        with open(file_name, 'r') as f:  
            lines = [line.strip() for line in f.readlines()]  
            prompt = lines[0]  
            answer = '\n'.join(lines[14:])  
            return prompt, answer  
    except Exception as e:  
        print(f"Error reading file: {e}")  
        sys.exit(1)  
  
def write_to_db(prompt, answer, v_prompt, v_answer, db_config):  
    try:  
        conn = psycopg2.connect(**db_config)  
        cursor = conn.cursor()  
        cursor.execute("INSERT INTO tbl (prompt, answer, v_prompt, v_answer) VALUES (%s, %s, %s, %s)", (prompt, answer, v_prompt.get("embedding"), v_answer.get("embedding")))  
        conn.commit()  
        conn.close()  
    except Exception as e:  
        print(f"Error writing to DB: {e}")  
        sys.exit(1)  
    finally:  
        # Close the cursor and connection  
        cursor.close()  
        conn.close()  
  
if __name__ == "__main__":  
    # Get the file path from the command line argument  
    if len(sys.argv) != 2:  
        print("Usage: python script.py <file_path>")  
        sys.exit(1)  
    file_path = sys.argv[1]  
  
    db_config = {  
        "host": HOST,  
        "port": PORT,  
        "database": DBNAME,  
        "user": USER,  
        "password": PASSWORD  
    }  
  
    prompt, answer = read_file(file_path)  
  
    v_prompt = ollama.embeddings(model='mxbai-embed-large', prompt=prompt)  
    v_answer = ollama.embeddings(model='mxbai-embed-large', prompt=answer)  
  
    write_to_db(prompt, answer, v_prompt, v_answer, db_config)  
```  
  
用法举例  
```  
python3 ./exp1.py /Users/digoal/blog/202407/20240717_02.md  
```  
  
写入正常, 生成向量维度1024.  
```  
postgres=# select prompt, vector_dims(v_prompt), vector_dims(v_answer) from tbl;  
                    prompt                     | vector_dims | vector_dims  
-----------------------------------------------+-------------+-------------  
 ## DuckDB官方推出"社区扩展插件市场", 未来可期 |        1024 |        1024  
(1 row)  
```  
  
实际上llama3也是可以生成embedding的, 只是更慢一点. 另外需要注意的是, 同一张表请使用同一个模型来生成embedding, 而且问题也要使用这个模型来生成然后再去该表进行向量相似搜索.   
  
如果你有很多的模型, 并且想对比不同模型的效果, 我建议你设计1张映射表, 存储模型和表的关系. 类似这样:   
```
create table t_mapping (id int primary key, model_name text, table_name text);

insert into t_mapping values (1, 'llama3:8b', 'tbl1'), (2, 'gemma2:9b', 'tbl2'), (3, 'phi3:medium', 'tbl3');  
   
create table tbl1 (...);
create table tbl2 (...);
create table tbl3 (...);
```
  
## 三、未完待续  
  
批量入库, 从向量数据库提取相关知识文章, 然后参考该文章进行回答.  
  
其他相关问题:   
- 分段切分整理问题.  
- 存储往期会话相关问题
- 如何微调模型.  
  
模型与数据的结合应用项目: https://github.com/mindsdb/mindsdb   
  
ollama 开发参考:   
- https://pypi.org/project/ollama/
- https://github.com/ollama/ollama-python
  
## 参考  
https://www.timescale.com/blog/use-open-source-llms-in-postgresql-with-ollama-and-pgai  
  
https://ollama.com/blog/embedding-models  
  
https://ollama.com/library/mxbai-embed-large  
  
https://github.com/timescale/pgai/  
  
  
#### [期望 PostgreSQL|开源PolarDB 增加什么功能?](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216")
  
  
#### [PolarDB 开源数据库](https://openpolardb.com/home "57258f76c37864c6e6d23383d05714ea")
  
  
#### [PolarDB 学习图谱](https://www.aliyun.com/database/openpolardb/activity "8642f60e04ed0c814bf9cb9677976bd4")
  
  
#### [购买PolarDB云服务折扣活动进行中, 55元起](https://www.aliyun.com/activity/new/polardb-yunparter?userCode=bsb3t4al "e0495c413bedacabb75ff1e880be465a")
  
  
#### [PostgreSQL 解决方案集合](../201706/20170601_02.md "40cff096e9ed7122c512b35d8561d9c8")
  
  
#### [德哥 / digoal's Github - 公益是一辈子的事.](https://github.com/digoal/blog/blob/master/README.md "22709685feb7cab07d30f30387f0a9ae")
  
  
#### [About 德哥](https://github.com/digoal/blog/blob/master/me/readme.md "a37735981e7704886ffd590565582dd0")
  
  
![digoal's wechat](../pic/digoal_weixin.jpg "f7ad92eeba24523fd47a6e1a0e691b59")
  
